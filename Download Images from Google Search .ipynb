{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a854609",
   "metadata": {},
   "source": [
    "# Download Images - Medical Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e33979",
   "metadata": {},
   "source": [
    "<!-- <hr> -->\n",
    "\n",
    "## Table of Content: <a class=\"anchor\" id=\"table-of-content\"></a>\n",
    "* [1. Problem Background](#problem-background)\n",
    "* [2. Import Package](#import-package)\n",
    "* [3. Custom Functions](#custom-functions)\n",
    "* [4. Download Images](#download-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ca234",
   "metadata": {},
   "source": [
    "## 1. Background and Motivation <a class=\"anchor\" id=\"problem-background\"></a>\n",
    "\n",
    "### Problem\n",
    "> Download images for medical devices. Downloaded images will work as training data for YOLO model to detect medical devices in the youtube video.\n",
    ">\n",
    "\n",
    "### Steps\n",
    "> - Use selenium and beautiful soup for web scrapping.\n",
    "> - Selenium will be used to initiate the web broswer and will scroll the web page to enable the javascript on the web page\n",
    "> - Beautiful soup is used to extract the html content from the web page.\n",
    "\n",
    "* [Go to Top](#table-of-content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba035d9",
   "metadata": {},
   "source": [
    "## 2. Import Libraries <a class=\"anchor\" id=\"import-package\"></a>\n",
    "<br>\n",
    "\n",
    "### Libraries<br>\n",
    "\n",
    ">**OS:**<br>\n",
    "We use it to create a new folder to save the image files  <br>\n",
    "\n",
    ">**Selenium and Beautiful Soup:**<br>\n",
    "We use the libraries for web scrapping  <br>\n",
    "\n",
    ">**PIL and io:**<br>\n",
    "These libraries are used to remove thumbnail images from the image data set <br>\n",
    "\n",
    ">**time:**<br>\n",
    "Time library is used to add the delay so that web page can be loaded <br>\n",
    "\n",
    "* [Go to Top](#table-of-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa4e82b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (4.8.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dhawa\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a128bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import *\n",
    "import requests\n",
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "# logging.basicConfig(filename=\"../Data/download_images_log.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f6821",
   "metadata": {},
   "source": [
    "## 3. Custom Functions <a class=\"anchor\" id=\"custom-functions\"></a>\n",
    "\n",
    "Glossary of User defined functions:\n",
    "1. **create_folder** - function to check if the folder is present or not. If folder is not present, then create the folder.\n",
    "2. **download_images** - function to download images from images from image URL.\n",
    "* [Go to Top](#table-of-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6e3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "#     folder_name='..\\Data\\Test'\n",
    "    isExist = os.path.exists(folder_name)\n",
    "    \n",
    "    if not isExist:\n",
    "        os.makedirs(folder_name)\n",
    "        print('New folder created')\n",
    "        logging.info('New folder created')\n",
    "    else:\n",
    "        print('Folder is already present')\n",
    "        logging.info('Folder is already present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90baff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_url(url):\n",
    "#     url = 'https://www.google.com/search?q=glucose+pen&tbm=isch&chips=q:glucose+pen,g_1:diabetes:jn6D-rZ6h10%3D&rlz=1C1RXQR_enUS1017US1017&hl=en&sa=X&ved=2ahUKEwiGuarJoIL-AhWPPkQIHQPEB_wQ4lYoAHoECAEQLA&biw=1381&bih=684'\n",
    "#     r = requests.get(url)\n",
    "#     soup= BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # open a web broser\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # open the url and expand to fullscreen\n",
    "    driver.get(url)\n",
    "    driver.fullscreen_window()\n",
    "    \n",
    "    # wait for page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "    load_more_images=1\n",
    "    show_more_ctr=0\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            show_more_ctr +=1\n",
    "            if show_more_ctr<=load_more_images:\n",
    "                element = driver.find_element(By.CLASS_NAME,'mye4qd')\n",
    "                if element:\n",
    "                    try:\n",
    "                        element.click()\n",
    "                    except:\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        last_height = new_height\n",
    "    \n",
    "    # extract source code of page after scrolling\n",
    "    page_source=driver.page_source\n",
    "    \n",
    "    # close the broser\n",
    "    driver.close()\n",
    "    \n",
    "    # get html part of page source\n",
    "    soup= BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "#     r = requests.get(url)\n",
    "#     soup= BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # get all the html image tags\n",
    "    images = soup.findAll('img')\n",
    "    \n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20e9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_image_url(url):\n",
    "# #     url = 'https://www.google.com/search?q=glucose+pen&tbm=isch&chips=q:glucose+pen,g_1:diabetes:jn6D-rZ6h10%3D&rlz=1C1RXQR_enUS1017US1017&hl=en&sa=X&ved=2ahUKEwiGuarJoIL-AhWPPkQIHQPEB_wQ4lYoAHoECAEQLA&biw=1381&bih=684'\n",
    "# #     r = requests.get(url)\n",
    "# #     soup= BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "#     # open a web broser\n",
    "#     driver = webdriver.Chrome()\n",
    "\n",
    "#     # open the url and expand to fullscreen\n",
    "#     driver.get(url)\n",
    "#     driver.fullscreen_window()\n",
    "    \n",
    "#     # wait for page to load\n",
    "#     time.sleep(5)\n",
    "\n",
    "#     # number of times page should be scrolled\n",
    "#     page_scroll=10\n",
    "    \n",
    "#     # scrolling the page enables the javascript of the webpage and helps to download more images\n",
    "#     for i in range(page_scroll):\n",
    "#         #page scroll\n",
    "#         driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#         # wait for page to load\n",
    "#         time.sleep(5)\n",
    "    \n",
    "#     # extract source code of page after scrolling\n",
    "#     page_source=driver.page_source\n",
    "    \n",
    "#     # close the broser\n",
    "# #     driver.close()\n",
    "    \n",
    "#     # get html part of page source\n",
    "#     soup= BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "# #     r = requests.get(url)\n",
    "# #     soup= BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "#     # get all the html image tags\n",
    "#     images = soup.findAll('img')\n",
    "    \n",
    "#     return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877a013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(folder_name,url):\n",
    "    count=0\n",
    "    \n",
    "    #get list of image urls\n",
    "    images = get_image_url(url)\n",
    "    \n",
    "    #remove duplicate image urls\n",
    "    images=list(set(images))\n",
    "    \n",
    "    #number of images related to the search query\n",
    "    total_images = len(images)\n",
    "\n",
    "    print(f'Total {total_images} Images Found!!')\n",
    "    logging.info(f'Total {total_images} Images Found!!')\n",
    "\n",
    "    if total_images !=0:\n",
    "        create_folder(folder_name)\n",
    "        \n",
    "        for i, image in enumerate(images[0:100]):\n",
    "\n",
    "            # first we will search for \"data-srcset\" in img tag\n",
    "            try:\n",
    "                # In image tag ,searching for \"data-srcset\"\n",
    "                image_link = image[\"data-srcset\"]\n",
    "\n",
    "            # then we will search for \"data-src\" in img\n",
    "            # tag and so on..\n",
    "            except:\n",
    "                try:\n",
    "                    # In image tag ,searching for \"data-src\"\n",
    "                    image_link = image[\"data-src\"]\n",
    "                except:\n",
    "                    try:\n",
    "                        # In image tag ,searching for \"data-fallback-src\"\n",
    "                        image_link = image[\"data-fallback-src\"]\n",
    "                    except:\n",
    "                        try:\n",
    "                            # In image tag ,searching for \"src\"\n",
    "                            image_link = image[\"src\"]\n",
    "\n",
    "                        # if no Source URL found\n",
    "                        except:\n",
    "                            pass\n",
    "    #         print(image_link)\n",
    "            try:\n",
    "                r = requests.get(image_link).content\n",
    "                \n",
    "                # capture size to remove thumbnails and very small images\n",
    "                im = Image.open(BytesIO(r))\n",
    "                width, height=im.size\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    r= str(r,'utf-8')\n",
    "\n",
    "                except UnicodeDecodeError:\n",
    "                    \n",
    "                    if width>50 and height >50:\n",
    "                        with open(f'{folder_name}/images_{i+1}.jpg','wb+') as f:\n",
    "                            f.write(r)\n",
    "\n",
    "                        count += 1\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print('No images found')\n",
    "        logging.error('No images found')\n",
    "\n",
    "    if count == total_images:\n",
    "        print(\"All Images Downloaded!\")\n",
    "        logging.info(f\"All Images Downloaded!\")\n",
    "\n",
    "    # if all images are not download\n",
    "    else:\n",
    "        print(f\"Total {count} Images Downloaded Out of {total_images}\")\n",
    "        logging.info(f\"Total {count} Images Downloaded Out of {total_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bc9a1",
   "metadata": {},
   "source": [
    "## 4. Download Images <a class=\"anchor\" id=\"download-images\"></a>\n",
    "\n",
    "\n",
    "* [Go to Top](#table-of-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1673c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # enter search query\n",
    "# searchTerm = \"Glucose Pen\"\n",
    "\n",
    "# # enter folder name to download images\n",
    "# searchTerm_Label='glucose_pen'\n",
    "# folder_name='..\\Data\\\\' + searchTerm_Label\n",
    "\n",
    "# # replace space with %20 \n",
    "# searchTerm = searchTerm.replace(' ','%20')\n",
    "\n",
    "# url = \"https://www.google.co.in/search?q=\"+searchTerm+\"&source=lnms&tbm=isch\"\n",
    "# # url = 'https://www.google.com/search?q=glucose+pen&tbm=isch&chips=q:glucose+pen,g_1:diabetes:jn6D-rZ6h10%3D&rlz=1C1RXQR_enUS1017US1017&hl=en&sa=X&ved=2ahUKEwiGuarJoIL-AhWPPkQIHQPEB_wQ4lYoAHoECAEQLA&biw=1381&bih=684'\n",
    "# download_images(folder_name,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cef885dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Object: donuts\n",
      "1  :  donuts\n",
      "Total 1322 Images Found!!\n",
      "New folder created\n",
      "Total 53 Images Downloaded Out of 1322\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# extract all the search queries\n",
    "df_search_query=pd.read_csv('SearchQuery.csv')\n",
    "df_search_query.head()\n",
    "\n",
    "df_search_query['Object'] = df_search_query['Object'].apply(lambda x: x.lower())\n",
    "df_search_query['Search Terms'] = df_search_query['Search Terms'].apply(lambda x: x.lower())\n",
    "\n",
    "# extract list of all the medical devices\n",
    "search_object=df_search_query['Object'].unique()\n",
    "# search_term = df_search_query['Search Terms']\n",
    "\n",
    "# loopthrough all the medical devices\n",
    "for medical_object in search_object:\n",
    "    print('Medical Object:',medical_object)\n",
    "    logging.info('Medical Object: ' + medical_object)\n",
    "    \n",
    "    search_terms = df_search_query[df_search_query['Object']==medical_object]['Search Terms'].unique()\n",
    "    # loop through all the search query related to particular medical device\n",
    "    for i, searchTerm in enumerate(search_terms[0:100]):\n",
    "        # search query\n",
    "        print(i+1,' : ',searchTerm)\n",
    "        logging.info(str(i+1)+' : '+searchTerm)\n",
    "    \n",
    "        # enter folder name to download images\n",
    "        searchTerm_Label=medical_object\n",
    "        folder_name='..\\Data\\\\' + medical_object+'\\\\'+searchTerm\n",
    "\n",
    "        # replace space with %20 \n",
    "        searchTerm = searchTerm.replace(' ','%20')\n",
    "\n",
    "        url = \"https://www.google.co.in/search?q=\"+searchTerm+\"&source=lnms&tbm=isch\"\n",
    "        # url = 'https://www.google.com/search?q=glucose+pen&tbm=isch&chips=q:glucose+pen,g_1:diabetes:jn6D-rZ6h10%3D&rlz=1C1RXQR_enUS1017US1017&hl=en&sa=X&ved=2ahUKEwiGuarJoIL-AhWPPkQIHQPEB_wQ4lYoAHoECAEQLA&biw=1381&bih=684'\n",
    "        download_images(folder_name,url)\n",
    "            \n",
    "    print('*'*30)\n",
    "    logging.info('-----------------------------------')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab6c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
